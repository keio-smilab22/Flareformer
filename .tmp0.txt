loading images ...
data/data_2010_label.csv
data/data_2011_label.csv
data/data_2012_label.csv
data/data_2013_label.csv
data/data_2010_feat.csv
data/data_2011_feat.csv
data/data_2012_feat.csv
data/data_2013_feat.csv
data/data_2010_window_48.csv
data/data_2011_window_48.csv
data/data_2012_window_48.csv
data/data_2013_window_48.csv
img: torch.Size([29247, 1, 256, 256]) label: (29247, 4) feat: (29247, 90) window: (29247, 4)
0.35716575 0.22504742444964324
job dir: /home/initial/Dropbox/flare_transformer/mae/prod
Namespace(accum_iter=1,
baseline='lambda',
batch_size=23,
batch_size_search=False,
blr=0.001,
device='cuda',
epochs=40,
input_size=256,
lr=0.0002,
mask_ratio=0.75,
min_lr=0.0,
model='vit_for_FT',
norm_pix_loss=False,
num_workers=10,
output_dir='output_dir',
pin_mem=True,
seed=0,
wandb=True,
warmup_epochs=40,
weight_decay=0.05)
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fc408e1c580>
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
MaskedAutoencoderViT                     --
├─PatchEmbed: 1-1                        --
│    └─Conv2d: 2-1                       33,280
│    └─Identity: 2-2                     --
├─ModuleList: 1-2                        --
│    └─Block: 2-3                        --
│    │    └─LayerNorm: 3-1               1,024
│    │    └─LambdaLayer: 3-2             108,192
│    │    └─Identity: 3-3                --
│    │    └─Identity: 3-4                --
│    │    └─LayerNorm: 3-5               1,024
│    │    └─Mlp: 3-6                     2,099,712
│    │    └─Identity: 3-7                --
│    │    └─Identity: 3-8                --
│    └─Block: 2-4                        --
│    │    └─LayerNorm: 3-9               1,024
│    │    └─LambdaLayer: 3-10            108,192
│    │    └─Identity: 3-11               --
│    │    └─Identity: 3-12               --
│    │    └─LayerNorm: 3-13              1,024
│    │    └─Mlp: 3-14                    2,099,712
│    │    └─Identity: 3-15               --
│    │    └─Identity: 3-16               --
│    └─Block: 2-5                        --
│    │    └─LayerNorm: 3-17              1,024
│    │    └─LambdaLayer: 3-18            108,192
│    │    └─Identity: 3-19               --
│    │    └─Identity: 3-20               --
│    │    └─LayerNorm: 3-21              1,024
│    │    └─Mlp: 3-22                    2,099,712
│    │    └─Identity: 3-23               --
│    │    └─Identity: 3-24               --
│    └─Block: 2-6                        --
│    │    └─LayerNorm: 3-25              1,024
│    │    └─LambdaLayer: 3-26            108,192
│    │    └─Identity: 3-27               --
│    │    └─Identity: 3-28               --
│    │    └─LayerNorm: 3-29              1,024
│    │    └─Mlp: 3-30                    2,099,712
│    │    └─Identity: 3-31               --
│    │    └─Identity: 3-32               --
│    └─Block: 2-7                        --
│    │    └─LayerNorm: 3-33              1,024
│    │    └─LambdaLayer: 3-34            108,192
│    │    └─Identity: 3-35               --
│    │    └─Identity: 3-36               --
│    │    └─LayerNorm: 3-37              1,024
│    │    └─Mlp: 3-38                    2,099,712
│    │    └─Identity: 3-39               --
│    │    └─Identity: 3-40               --
│    └─Block: 2-8                        --
│    │    └─LayerNorm: 3-41              1,024
│    │    └─LambdaLayer: 3-42            108,192
│    │    └─Identity: 3-43               --
│    │    └─Identity: 3-44               --
│    │    └─LayerNorm: 3-45              1,024
│    │    └─Mlp: 3-46                    2,099,712
│    │    └─Identity: 3-47               --
│    │    └─Identity: 3-48               --
│    └─Block: 2-9                        --
│    │    └─LayerNorm: 3-49              1,024
│    │    └─LambdaLayer: 3-50            108,192
│    │    └─Identity: 3-51               --
│    │    └─Identity: 3-52               --
│    │    └─LayerNorm: 3-53              1,024
│    │    └─Mlp: 3-54                    2,099,712
│    │    └─Identity: 3-55               --
│    │    └─Identity: 3-56               --
│    └─Block: 2-10                       --
│    │    └─LayerNorm: 3-57              1,024
│    │    └─LambdaLayer: 3-58            108,192
│    │    └─Identity: 3-59               --
│    │    └─Identity: 3-60               --
│    │    └─LayerNorm: 3-61              1,024
│    │    └─Mlp: 3-62                    2,099,712
│    │    └─Identity: 3-63               --
│    │    └─Identity: 3-64               --
│    └─Block: 2-11                       --
│    │    └─LayerNorm: 3-65              1,024
│    │    └─LambdaLayer: 3-66            108,192
│    │    └─Identity: 3-67               --
│    │    └─Identity: 3-68               --
│    │    └─LayerNorm: 3-69              1,024
│    │    └─Mlp: 3-70                    2,099,712
│    │    └─Identity: 3-71               --
│    │    └─Identity: 3-72               --
│    └─Block: 2-12                       --
│    │    └─LayerNorm: 3-73              1,024
│    │    └─LambdaLayer: 3-74            108,192
│    │    └─Identity: 3-75               --
│    │    └─Identity: 3-76               --
│    │    └─LayerNorm: 3-77              1,024
│    │    └─Mlp: 3-78                    2,099,712
│    │    └─Identity: 3-79               --
│    │    └─Identity: 3-80               --
│    └─Block: 2-13                       --
│    │    └─LayerNorm: 3-81              1,024
│    │    └─LambdaLayer: 3-82            108,192
│    │    └─Identity: 3-83               --
│    │    └─Identity: 3-84               --
│    │    └─LayerNorm: 3-85              1,024
│    │    └─Mlp: 3-86                    2,099,712
│    │    └─Identity: 3-87               --
│    │    └─Identity: 3-88               --
│    └─Block: 2-14                       --
│    │    └─LayerNorm: 3-89              1,024
│    │    └─LambdaLayer: 3-90            108,192
│    │    └─Identity: 3-91               --
│    │    └─Identity: 3-92               --
│    │    └─LayerNorm: 3-93              1,024
│    │    └─Mlp: 3-94                    2,099,712
│    │    └─Identity: 3-95               --
│    │    └─Identity: 3-96               --
├─LayerNorm: 1-3                         1,024
├─Linear: 1-4                            262,656
├─ModuleList: 1-5                        --
│    └─Block: 2-15                       --
│    │    └─LayerNorm: 3-97              1,024
│    │    └─LambdaLayer: 3-98            108,192
│    │    └─Identity: 3-99               --
│    │    └─Identity: 3-100              --
│    │    └─LayerNorm: 3-101             1,024
│    │    └─Mlp: 3-102                   2,099,712
│    │    └─Identity: 3-103              --
│    │    └─Identity: 3-104              --
│    └─Block: 2-16                       --
│    │    └─LayerNorm: 3-105             1,024
│    │    └─LambdaLayer: 3-106           108,192
│    │    └─Identity: 3-107              --
│    │    └─Identity: 3-108              --
│    │    └─LayerNorm: 3-109             1,024
│    │    └─Mlp: 3-110                   2,099,712
│    │    └─Identity: 3-111              --
│    │    └─Identity: 3-112              --
│    └─Block: 2-17                       --
│    │    └─LayerNorm: 3-113             1,024
│    │    └─LambdaLayer: 3-114           108,192
│    │    └─Identity: 3-115              --
│    │    └─Identity: 3-116              --
│    │    └─LayerNorm: 3-117             1,024
│    │    └─Mlp: 3-118                   2,099,712
│    │    └─Identity: 3-119              --
│    │    └─Identity: 3-120              --
│    └─Block: 2-18                       --
│    │    └─LayerNorm: 3-121             1,024
│    │    └─LambdaLayer: 3-122           108,192
│    │    └─Identity: 3-123              --
│    │    └─Identity: 3-124              --
│    │    └─LayerNorm: 3-125             1,024
│    │    └─Mlp: 3-126                   2,099,712
│    │    └─Identity: 3-127              --
│    │    └─Identity: 3-128              --
│    └─Block: 2-19                       --
│    │    └─LayerNorm: 3-129             1,024
│    │    └─LambdaLayer: 3-130           108,192
│    │    └─Identity: 3-131              --
│    │    └─Identity: 3-132              --
│    │    └─LayerNorm: 3-133             1,024
│    │    └─Mlp: 3-134                   2,099,712
│    │    └─Identity: 3-135              --
│    │    └─Identity: 3-136              --
│    └─Block: 2-20                       --
│    │    └─LayerNorm: 3-137             1,024
│    │    └─LambdaLayer: 3-138           108,192
│    │    └─Identity: 3-139              --
│    │    └─Identity: 3-140              --
│    │    └─LayerNorm: 3-141             1,024
│    │    └─Mlp: 3-142                   2,099,712
│    │    └─Identity: 3-143              --
│    │    └─Identity: 3-144              --
│    └─Block: 2-21                       --
│    │    └─LayerNorm: 3-145             1,024
│    │    └─LambdaLayer: 3-146           108,192
│    │    └─Identity: 3-147              --
│    │    └─Identity: 3-148              --
│    │    └─LayerNorm: 3-149             1,024
│    │    └─Mlp: 3-150                   2,099,712
│    │    └─Identity: 3-151              --
│    │    └─Identity: 3-152              --
│    └─Block: 2-22                       --
│    │    └─LayerNorm: 3-153             1,024
│    │    └─LambdaLayer: 3-154           108,192
│    │    └─Identity: 3-155              --
│    │    └─Identity: 3-156              --
│    │    └─LayerNorm: 3-157             1,024
│    │    └─Mlp: 3-158                   2,099,712
│    │    └─Identity: 3-159              --
│    │    └─Identity: 3-160              --
├─LayerNorm: 1-6                         1,024
├─Linear: 1-7                            32,832
=================================================================
Total params: 44,529,856
Trainable params: 44,529,856
Non-trainable params: 0
=================================================================
base lr: 2.23e-03
actual lr: 2.00e-04
accumulate grad iterations: 1
effective batch size: 23
Start training for 40 epochs
====== Epoch  1  ======
time: 433.353s
====== Epoch  2  ======
time: 432.276s
====== Epoch  3  ======
time: 433.131s
====== Epoch  4  ======
time: 432.974s
====== Epoch  5  ======
time: 432.780s
====== Epoch  6  ======
time: 432.372s
====== Epoch  7  ======
time: 432.597s
====== Epoch  8  ======
time: 432.789s
====== Epoch  9  ======
time: 432.486s
====== Epoch  10  ======
time: 432.076s
====== Epoch  11  ======
time: 432.760s
====== Epoch  12  ======
time: 432.430s
====== Epoch  13  ======
time: 433.018s
====== Epoch  14  ======
time: 432.877s
====== Epoch  15  ======
time: 432.643s
====== Epoch  16  ======
time: 432.250s
====== Epoch  17  ======
time: 432.496s
====== Epoch  18  ======
time: 432.987s
====== Epoch  19  ======
time: 432.696s
====== Epoch  20  ======
time: 433.445s
====== Epoch  21  ======
time: 432.909s
====== Epoch  22  ======
time: 432.399s
====== Epoch  23  ======
time: 432.355s
====== Epoch  24  ======
time: 432.728s
====== Epoch  25  ======
time: 433.412s
====== Epoch  26  ======
time: 432.318s
====== Epoch  27  ======
time: 432.764s
====== Epoch  28  ======
time: 432.613s
====== Epoch  29  ======
time: 433.076s
====== Epoch  30  ======
time: 433.333s
====== Epoch  31  ======
time: 432.432s
====== Epoch  32  ======
time: 433.126s
====== Epoch  33  ======
time: 432.725s
====== Epoch  34  ======
time: 433.574s
====== Epoch  35  ======
time: 432.672s
====== Epoch  36  ======
time: 432.978s
====== Epoch  37  ======
time: 433.037s
====== Epoch  38  ======
time: 432.776s
====== Epoch  39  ======
time: 432.924s
====== Epoch  40  ======
time: 432.905s
Training time 4:48:36

